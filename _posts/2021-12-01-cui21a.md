---
title: Minimax sample complexity for turn-based stochastic game
abstract: The empirical success of multi-agent reinforcement learning is encouraging,
  while few theoretical guarantees have been revealed. In this work, we prove that
  the plug-in solver approach, probably the most natural reinforcement learning algorithm,
  achieves minimax sample complexity for turn-based stochastic game (TBSG). Specifically,
  we perform planning in an empirical TBSG by utilizing a ‘simulator’ that allows
  sampling from arbitrary state-action pair. We show that the empirical Nash equilibrium
  strategy is an approximate Nash equilibrium strategy in the true TBSG and give both
  problem-dependent and problem-independent bound. We develop reward perturbation
  techniques to tackle the non-stationarity in the game and Taylor-expansion-type
  analysis to improve the dependence on approximation error. With these novel techniques,
  we prove the minimax sample complexity of turn-based stochastic game.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cui21a
month: 0
tex_title: Minimax sample complexity for turn-based stochastic game
firstpage: 1496
lastpage: 1504
page: 1496-1504
order: 1496
cycles: false
bibtex_author: Cui, Qiwen and Yang, Lin F.
author:
- given: Qiwen
  family: Cui
- given: Lin F.
  family: Yang
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/cui21a/cui21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/cui21a/cui21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
