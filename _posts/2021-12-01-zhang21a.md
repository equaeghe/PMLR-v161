---
title: On the distributional properties of adaptive gradients
abstract: Adaptive gradient methods have achieved remarkable success in training deep
  neural networks on a wide variety of tasks. However, not much is known about the
  mathematical and statistical properties of this family of methods. This work aims
  at providing a series of theoretical analyses of its statistical properties justified
  by experiments. In particular, we show that when the underlying gradient obeys a
  normal distribution, the variance of the magnitude of the <em>update</em> is an
  increasing and bounded function of time and does not diverge. This work suggests
  that the divergence of variance is not the cause of the need for warm-up of the
  Adam optimizer, contrary to what is believed in the current literature.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21a
month: 0
tex_title: On the distributional properties of adaptive gradients
firstpage: 419
lastpage: 429
page: 419-429
order: 419
cycles: false
bibtex_author: Zhang, Zhiyi and Liu, Ziyin
author:
- given: Zhiyi
  family: Zhang
- given: Ziyin
  family: Liu
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/zhang21a/zhang21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/zhang21a/zhang21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
