---
title: Constrained labeling for weakly supervised learning
abstract: Curation of large fully supervised datasets has become one of the major
  roadblocks for machine learning. Weak supervision provides an alternative to supervised
  learning by training with cheap, noisy, and possibly correlated labeling functions
  from varying sources. The key challenge in weakly supervised learning is combining
  the different weak supervision signals while navigating misleading correlations
  in their errors. In this paper, we propose a simple data-free approach for combining
  weak supervision signals by defining a constrained space for the possible labels
  of the weak signals and training with a random labeling within this constrained
  space. Our method is efficient and stable, converging after a few iterations of
  gradient descent. We prove theoretical conditions under which the worst-case error
  of the randomized label decreases with the rank of the linear constraints. We show
  experimentally that our method outperforms other weak supervision methods on various
  text- and image-classification tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: arachie21a
month: 0
tex_title: Constrained labeling for weakly supervised learning
firstpage: 236
lastpage: 246
page: 236-246
order: 236
cycles: false
bibtex_author: Arachie, Chidubem and Huang, Bert
author:
- given: Chidubem
  family: Arachie
- given: Bert
  family: Huang
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/arachie21a/arachie21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
