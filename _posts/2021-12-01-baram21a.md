---
title: Action redundancy in reinforcement learning
abstract: Maximum Entropy (MaxEnt) reinforcement learning is a powerful learning paradigm
  which seeks to maximize return under entropy regularization. However, action entropy
  does not necessarily coincide with state entropy, e.g., when multiple actions produce
  the same transition. Instead, we propose to maximize the transition entropy, i.e.,
  the entropy of next states. We show that transition entropy can be described by
  two terms; namely, model-dependent transition entropy and <b>action redundancy</b>.
  Particularly, we explore the latter in both deterministic and stochastic settings
  and develop tractable approximation methods in a near model-free setup. We construct
  algorithms to minimize action redundancy and demonstrate their effectiveness on
  a synthetic environment with multiple redundant actions as well as contemporary
  benchmarks in Atari and Mujoco. Our results suggest that action redundancy is a
  fundamental problem in reinforcement learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: baram21a
month: 0
tex_title: Action redundancy in reinforcement learning
firstpage: 376
lastpage: 385
page: 376-385
order: 376
cycles: false
bibtex_author: Baram, Nir and Tennenholtz, Guy and Mannor, Shie
author:
- given: Nir
  family: Baram
- given: Guy
  family: Tennenholtz
- given: Shie
  family: Mannor
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/baram21a/baram21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/baram21a/baram21a-supp.pdf
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v161/baram21a/baram21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
