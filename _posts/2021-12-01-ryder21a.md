---
title: The neural moving average model for scalable variational inference of state
  space models
abstract: 'Variational inference has had great success in scaling approximate Bayesian
  inference to big data by exploiting mini-batch training. To date, however, this
  strategy has been most applicable to models of independent data. We propose an extension
  to state space models of time series data based on a novel generative model for
  latent temporal states: the neural moving average model. This permits a subsequence
  to be sampled without drawing from the entire distribution, enabling training iterations
  to use mini-batches of the time series at low computational cost. We illustrate
  our method on autoregressive, Lotka-Volterra, FitzHugh-Nagumo and stochastic volatility
  models, achieving accurate parameter estimation in a short time.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ryder21a
month: 0
tex_title: The neural moving average model for scalable variational inference of state
  space models
firstpage: 12
lastpage: 22
page: 12-22
order: 12
cycles: false
bibtex_author: Ryder, Thomas and Prangle, Dennis and Golightly, Andrew and Matthews,
  Isaac
author:
- given: Thomas
  family: Ryder
- given: Dennis
  family: Prangle
- given: Andrew
  family: Golightly
- given: Isaac
  family: Matthews
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/ryder21a/ryder21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/ryder21a/ryder21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
