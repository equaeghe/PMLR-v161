---
title: Hierarchical Indian buffet neural networks for Bayesian continual learning
abstract: We place an Indian Buffet process (IBP) prior over the structure of a Bayesian
  Neural Network (BNN), thus allowing the complexity of the BNN to increase and decrease
  automatically. We further extend this model such that the prior on the structure
  of each hidden layer is shared globally across all layers, using a Hierarchical-IBP
  (H-IBP). We apply this model to the problem of resource allocation in Continual
  Learning (CL) where new tasks occur and the network requires extra resources. Our
  model uses online variational inference with reparameterisation of the Bernoulli
  and Beta distributions, which constitute the IBP and H-IBP priors. As we automatically
  learn the number of weights in each layer of the BNN, overfitting and underfitting
  problems are largely overcome. We show empirically that our approach offers a competitive
  edge over existing methods in CL.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kessler21a
month: 0
tex_title: Hierarchical Indian buffet neural networks for Bayesian continual learning
firstpage: 749
lastpage: 759
page: 749-759
order: 749
cycles: false
bibtex_author: Kessler, Samuel and Nguyen, Vu and Zohren, Stefan and Roberts, Stephen
  J.
author:
- given: Samuel
  family: Kessler
- given: Vu
  family: Nguyen
- given: Stefan
  family: Zohren
- given: Stephen J.
  family: Roberts
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/kessler21a/kessler21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/kessler21a/kessler21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
