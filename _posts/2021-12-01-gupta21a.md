---
title: 'LocalNewton: Reducing communication rounds for distributed learning'
abstract: To address the communication bottleneck problem in distributed optimization
  within a master-worker framework, we propose LocalNewton, a distributed second-order
  algorithm with <em>local averaging</em>. In LocalNewton, the worker machines update
  their model in every iteration by finding a suitable second-order descent direction
  using only the data and model stored in their own local memory. We let the workers
  run multiple such iterations locally and communicate the models to the master node
  only once every few (say $L$) iterations. LocalNewton is highly practical since
  it requires only one hyperparameter, the number $L$ of local iterations. We use
  novel matrix concentration based techniques to obtain theoretical guarantees for
  LocalNewton, and we validate them with detailed empirical evaluation. To enhance
  practicability, we devise an adaptive scheme to choose $L$, and we show that this
  reduces the number of local iterations in worker machines between two model synchronizations
  as the training proceeds, successively refining the model quality at the master.
  Via extensive experiments using several real-world datasets with AWS Lambda workers
  and an AWS EC2 master, we show that LocalNewton requires fewer than $60%$ of the
  communication rounds (between master and workers) and less than $40%$ of the end-to-end
  running time, compared to state-of-the-art algorithms, to reach the same training
  loss.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gupta21a
month: 0
tex_title: 'LocalNewton: Reducing communication rounds for distributed learning'
firstpage: 632
lastpage: 642
page: 632-642
order: 632
cycles: false
bibtex_author: Gupta, Vipul and Ghosh, Avishek and Derezi\'nski, Micha{\l} and Khanna,
  Rajiv and Ramchandran, Kannan and Mahoney, Michael W.
author:
- given: Vipul
  family: Gupta
- given: Avishek
  family: Ghosh
- given: Michał
  family: Dereziński
- given: Rajiv
  family: Khanna
- given: Kannan
  family: Ramchandran
- given: Michael W.
  family: Mahoney
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/gupta21a/gupta21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/gupta21a/gupta21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
