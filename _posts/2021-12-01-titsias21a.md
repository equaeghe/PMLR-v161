---
title: Information theoretic meta learning with Gaussian processes
abstract: We formulate meta learning using information theoretic concepts; namely,
  mutual information and the information bottleneck. The idea is to learn a stochastic
  representation or encoding of the task description, given by a training set, that
  is highly informative about predicting the validation set. By making use of variational
  approximations to the mutual information, we derive a general and tractable framework
  for meta learning. This framework unifies existing gradient-based algorithms and
  also allows us to derive new algorithms. In particular, we develop a memory-based
  algorithm that uses Gaussian processes to obtain non-parametric encoding representations.
  We demonstrate our method on a few-shot regression problem and on four few-shot
  classification problems, obtaining competitive accuracy when compared to existing
  baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: titsias21a
month: 0
tex_title: Information theoretic meta learning with {G}aussian processes
firstpage: 1597
lastpage: 1606
page: 1597-1606
order: 1597
cycles: false
bibtex_author: Titsias, Michalis K. and Ruiz, Francisco J. R. and Nikoloutsopoulos,
  Sotirios and Galashov, Alexandre
author:
- given: Michalis K.
  family: Titsias
- given: Francisco J. R.
  family: Ruiz
- given: Sotirios
  family: Nikoloutsopoulos
- given: Alexandre
  family: Galashov
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/titsias21a/titsias21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/titsias21a/titsias21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
