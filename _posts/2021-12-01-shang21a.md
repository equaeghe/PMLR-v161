---
title: Principal component analysis in the stochastic differential privacy model
abstract: In this paper, we study the differentially private Principal Component Analysis
  (PCA) problem in stochastic optimization settings. We first propose a new stochastic
  gradient perturbation PCA mechanism (DP-SPCA) for the calculation of the right singular
  subspace to achieve $(\epsilon,\delta)$-differential privacy. For achieving a better
  utility guarantee and performance, we then present a new differential privacy stochastic
  variance reduction mechanism (DP-VRPCA) with gradient perturbation for PCA. To the
  best of our knowledge, this is the first work of stochastic gradient perturbation
  for $(\epsilon,\delta)$-differentially private PCA. We also compare the proposed
  algorithms with existing state-of-the-art methods, and experiments on real-world
  datasets and on classification tasks confirm the improved theoretical guarantees
  of our algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shang21a
month: 0
tex_title: Principal component analysis in the stochastic differential privacy model
firstpage: 1110
lastpage: 1119
page: 1110-1119
order: 1110
cycles: false
bibtex_author: Shang, Fanhua and Zhang, Zhihui and Xu, Tao and Liu, Yuanyuan and Liu,
  Hongying
author:
- given: Fanhua
  family: Shang
- given: Zhihui
  family: Zhang
- given: Tao
  family: Xu
- given: Yuanyuan
  family: Liu
- given: Hongying
  family: Liu
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/shang21a/shang21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
