---
title: On random kernels of residual architectures
abstract: We analyze the finite corrections to the neural tangent kernel (NTK) of
  residual and densely connected networks, as a function of both depth and width.
  Surprisingly, our analysis reveals that given a fixed depth, residual networks provide
  the best tradeoff between the parameter complexity and the coefficient of variation
  (normalized variance), followed by densely connected networks and vanilla MLPs.
  While in networks that do not use skip connections, convergence to the NTK requires
  one to fix the depth, while increasing the layersâ€™ width. Our findings show that
  in ResNets, convergence to the NTK may occur when depth and width simultaneously
  tend to infinity, provided with a proper initialization. In DenseNets, however,
  the convergence of the NTK to its limit as the width tends to infinity is guaranteed,
  at a rate that is independent of both the depth and scale of the weights. Our experiments
  validate the theoretical results and demonstrate the advantage of deep ResNets and
  DenseNets for kernel regression with random gradient features.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: littwin21a
month: 0
tex_title: On random kernels of residual architectures
firstpage: 897
lastpage: 907
page: 897-907
order: 897
cycles: false
bibtex_author: Littwin, Etai and Galanti, Tomer and Wolf, Lior
author:
- given: Etai
  family: Littwin
- given: Tomer
  family: Galanti
- given: Lior
  family: Wolf
date: 2021-12-01
address:
container-title: Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial
  Intelligence
volume: '161'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 12
  - 1
pdf: https://proceedings.mlr.press/v161/littwin21a/littwin21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v161/littwin21a/littwin21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
